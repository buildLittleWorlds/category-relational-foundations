{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Tutorial 9: Weighted Passages\n",
    "## Enriched Categories and Probabilistic Structure\n",
    "\n",
    "---\n",
    "\n",
    "### Beyond Yes and No\n",
    "\n",
    "*To the research assistant:*\n",
    "\n",
    "*In Year 901, Tessery Vold made a crucial refinement to her passage diagrams. The original framework asked only: \"Does a passage exist from A to B?\" But observation revealed a richer structure.*\n",
    "\n",
    "*Not all passages are equal. Stakdurs traverse from territory to reed marsh daily—this is a well-worn path. But the passage from deep Dens to Capital outskirts is rare, observed perhaps once in a decade. To capture this, Vold proposed \"weighted passages\"—morphisms decorated with degrees.*\n",
    "\n",
    "*Instead of Hom(A, B) being a set (either empty or containing morphisms), Vold proposed that Hom(A, B) could be a number: a probability, a frequency, a cost. This transforms the passage category into what modern mathematicians call an \"enriched category.\"*\n",
    "\n",
    "*Marden Krell grudgingly admitted this was an improvement: \"At least now we can distinguish common events from rare ones. But you still have not explained why these passages exist at all.\"*\n",
    "\n",
    "*Your task: Analyze weighted passage data to understand enriched categorical structure. Determine how weights affect composition, identity, and the overall behavior of the category.*\n",
    "\n",
    "—*Archive Review Committee, Year 934*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## What You Will Learn\n",
    "\n",
    "In this tutorial, you will learn to:\n",
    "\n",
    "1. Understand **enriched categories**: morphisms with values, not just existence\n",
    "2. Model **probabilistic morphisms** and their composition\n",
    "3. Connect enrichment to **weighted graphs** and **Markov chains**\n",
    "4. See how enriched categories relate to **attention mechanisms** in neural networks\n",
    "5. Apply weighted reasoning to passage diagram data\n",
    "\n",
    "By the end, you will understand:\n",
    "- Why \"how much\" matters as much as \"whether\"\n",
    "- The categorical foundation of probabilistic models\n",
    "- How attention mechanisms can be viewed as enriched functors\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## What Is an Enriched Category?\n",
    "\n",
    "In an ordinary category:\n",
    "- Hom(A, B) is a **set** of morphisms\n",
    "\n",
    "In an enriched category:\n",
    "- Hom(A, B) is an **object in some other category V**\n",
    "\n",
    "Common enrichments:\n",
    "- **V = [0, ∞]**: Hom(A, B) = distance/cost from A to B (metric spaces)\n",
    "- **V = [0, 1]**: Hom(A, B) = probability of passing from A to B\n",
    "- **V = ℝ**: Hom(A, B) = similarity or weight between A and B\n",
    "- **V = Bool**: Hom(A, B) = true/false (this recovers pre-orders)\n",
    "\n",
    "### Composition in Enriched Categories\n",
    "\n",
    "Composition must respect the enrichment:\n",
    "- For distances: Hom(A, C) ≤ Hom(A, B) + Hom(B, C) (triangle inequality)\n",
    "- For probabilities: Hom(A, C) ≥ Hom(A, B) × Hom(B, C) (chain rule)\n",
    "\n",
    "### Vold's Interpretation\n",
    "\n",
    "> *\"A passage is not merely a yes or no. The stakdur's daily hunt is a certainty; the grimslew's rare emergence is a whisper. To capture the world's structure, we must weight our passages.\"*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Part 1: Loading and Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('deep')\n",
    "\n",
    "print(\"Libraries loaded. Ready to study weighted passages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load passage diagram data\n",
    "BASE_URL = \"https://raw.githubusercontent.com/buildLittleWorlds/densworld-datasets/main/data/\"\n",
    "\n",
    "passages = pd.read_csv(BASE_URL + \"passage_diagrams.csv\")\n",
    "\n",
    "print(f\"Passages loaded: {len(passages)} morphisms\")\n",
    "print(f\"\\nColumns: {list(passages.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 'frequency' column provides weight information\n",
    "print(\"Frequency values in data:\")\n",
    "print(passages['frequency'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "The frequency values describe how often passages occur. Let's convert these to numerical weights for our enriched category.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Part 2: Converting to Numerical Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a mapping from frequency descriptions to probabilities\n",
    "frequency_to_probability = {\n",
    "    'constant': 1.0,      # Always happens\n",
    "    'daily': 0.95,        # Very frequent\n",
    "    'weekly': 0.8,        # Frequent\n",
    "    'dawn': 0.85,         # Regular timing\n",
    "    'dusk': 0.85,\n",
    "    'circadian': 0.9,     # Daily cycle\n",
    "    'tidal': 0.75,        # Periodic\n",
    "    'overnight': 0.7,\n",
    "    'monthly': 0.5,       # Moderate\n",
    "    'seasonal': 0.4,      # Less frequent\n",
    "    'annual': 0.3,\n",
    "    'per_submission': 0.6,\n",
    "    'per_acceptance': 0.5,\n",
    "    'achievement': 0.3,   # Milestone events\n",
    "    'production': 0.7,\n",
    "    'developmental': 0.5,\n",
    "    'maturity': 0.3,\n",
    "    'inductive': 0.6,\n",
    "    'deductive': 0.5,\n",
    "    'confirmatory': 0.4,\n",
    "    'interactive': 0.7,\n",
    "    'variable': 0.3,      # Unpredictable\n",
    "    'episodic': 0.2,\n",
    "    'cascading': 0.15,\n",
    "    'rare': 0.1,          # Very rare\n",
    "    'irreversible': 0.05, # One-time\n",
    "    'mortality': 0.02,\n",
    "    'terminal': 0.01,\n",
    "    'temporal': 0.3,\n",
    "    'never': 0.0,         # Impossible\n",
    "    'unknown': 0.5        # Default to moderate\n",
    "}\n",
    "\n",
    "# Add probability column\n",
    "passages['weight'] = passages['frequency'].map(\n",
    "    lambda x: frequency_to_probability.get(x, 0.5)\n",
    ")\n",
    "\n",
    "print(\"Weight distribution:\")\n",
    "print(passages['weight'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize weight distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "passages['weight'].hist(bins=20, ax=ax, color='steelblue', alpha=0.7)\n",
    "ax.set_xlabel('Weight (Probability)')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Distribution of Passage Weights')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "Now we have a numerical weight for each passage. This is our enriched morphism: not just \"does a passage exist?\" but \"how probable/frequent is it?\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Part 3: Building the Enriched Category\n",
    "\n",
    "In an enriched category, Hom(A, B) is a value (here, a probability), not a set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the enriched Hom structure\n",
    "# For simplicity, if multiple morphisms exist between A and B, take the max probability\n",
    "\n",
    "all_objects = sorted(set(passages['source_object']) | set(passages['target_object']))\n",
    "n = len(all_objects)\n",
    "obj_to_idx = {obj: i for i, obj in enumerate(all_objects)}\n",
    "\n",
    "# Initialize Hom matrix with 0 (no passage)\n",
    "Hom = np.zeros((n, n))\n",
    "\n",
    "# Fill in from data\n",
    "for _, row in passages.iterrows():\n",
    "    i = obj_to_idx[row['source_object']]\n",
    "    j = obj_to_idx[row['target_object']]\n",
    "    Hom[i, j] = max(Hom[i, j], row['weight'])\n",
    "\n",
    "# Set identity morphisms to 1 (certainty)\n",
    "np.fill_diagonal(Hom, 1.0)\n",
    "\n",
    "print(f\"Enriched Hom matrix: {Hom.shape}\")\n",
    "print(f\"Non-zero entries: {np.count_nonzero(Hom)} (including {n} identities)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a portion of the Hom matrix\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# Show first 15 objects\n",
    "k = 15\n",
    "sns.heatmap(Hom[:k, :k], annot=True, fmt='.2f', cmap='Blues',\n",
    "            xticklabels=all_objects[:k], yticklabels=all_objects[:k],\n",
    "            ax=ax)\n",
    "ax.set_title('Enriched Hom Matrix (Probability of Passage)\\nFirst 15 Objects')\n",
    "ax.set_xlabel('Target')\n",
    "ax.set_ylabel('Source')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "Each entry Hom[i, j] is the probability/weight of passing from object i to object j. The diagonal is all 1s (identity morphisms have weight 1).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Part 4: Composition in Enriched Categories\n",
    "\n",
    "In a probability-enriched category, composition is multiplication:\n",
    "\n",
    "If P(A → B) = p and P(B → C) = q, then P(A → C via B) = p × q\n",
    "\n",
    "But there may be multiple paths, so we often take max:\n",
    "\n",
    "Hom(A, C) = max over all B of [Hom(A, B) × Hom(B, C)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enriched_composition(Hom, steps=1):\n",
    "    \"\"\"\n",
    "    Compute the composed Hom matrix after 'steps' compositions.\n",
    "    Uses max-product semiring: (max, ×)\n",
    "    \"\"\"\n",
    "    result = Hom.copy()\n",
    "    for _ in range(steps):\n",
    "        new_result = np.zeros_like(result)\n",
    "        for i in range(len(result)):\n",
    "            for j in range(len(result)):\n",
    "                # Max over all intermediate objects\n",
    "                max_path = max(result[i, k] * Hom[k, j] for k in range(len(result)))\n",
    "                new_result[i, j] = max(result[i, j], max_path)\n",
    "        result = new_result\n",
    "    return result\n",
    "\n",
    "# Compute 2-step composition\n",
    "Hom_2 = enriched_composition(Hom, steps=1)\n",
    "\n",
    "print(\"After one round of composition:\")\n",
    "print(f\"  Original non-zero entries: {np.count_nonzero(Hom)}\")\n",
    "print(f\"  After composition: {np.count_nonzero(Hom_2 > 0.01)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find new reachable pairs after composition\n",
    "new_pairs = []\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        if Hom[i, j] < 0.01 and Hom_2[i, j] > 0.01:\n",
    "            new_pairs.append((all_objects[i], all_objects[j], Hom_2[i, j]))\n",
    "\n",
    "new_pairs.sort(key=lambda x: -x[2])\n",
    "\n",
    "print(f\"New passages discovered through composition: {len(new_pairs)}\")\n",
    "print(\"\\nTop 10 by probability:\")\n",
    "for src, tgt, prob in new_pairs[:10]:\n",
    "    print(f\"  {src} → {tgt}: {prob:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "Composition in the enriched category reveals indirect passages that weren't in the original data but can be inferred from chains.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Part 5: The Weighted Graph Perspective\n",
    "\n",
    "An enriched category is like a weighted directed graph. Let's build one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build weighted graph\n",
    "G_weighted = nx.DiGraph()\n",
    "\n",
    "# Add nodes\n",
    "G_weighted.add_nodes_from(all_objects)\n",
    "\n",
    "# Add weighted edges\n",
    "for _, row in passages.iterrows():\n",
    "    G_weighted.add_edge(\n",
    "        row['source_object'], \n",
    "        row['target_object'],\n",
    "        weight=row['weight'],\n",
    "        frequency=row['frequency']\n",
    "    )\n",
    "\n",
    "print(f\"Weighted graph: {G_weighted.number_of_nodes()} nodes, {G_weighted.number_of_edges()} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find highest-weight paths\n",
    "def highest_weight_path(G, source, target):\n",
    "    \"\"\"Find the path with highest product of weights.\"\"\"\n",
    "    if source == target:\n",
    "        return [source], 1.0\n",
    "    \n",
    "    try:\n",
    "        # Find all simple paths\n",
    "        best_path = None\n",
    "        best_weight = 0\n",
    "        \n",
    "        for path in nx.all_simple_paths(G, source, target, cutoff=4):\n",
    "            # Compute product of edge weights\n",
    "            weight = 1.0\n",
    "            for i in range(len(path) - 1):\n",
    "                edge_data = G.get_edge_data(path[i], path[i+1])\n",
    "                if edge_data:\n",
    "                    weight *= edge_data.get('weight', 0)\n",
    "            \n",
    "            if weight > best_weight:\n",
    "                best_weight = weight\n",
    "                best_path = path\n",
    "        \n",
    "        return best_path, best_weight\n",
    "    except nx.NetworkXNoPath:\n",
    "        return None, 0.0\n",
    "\n",
    "# Test on some pairs\n",
    "test_pairs = [\n",
    "    ('stakdur_territory', 'reed_marsh'),\n",
    "    ('deep_dens', 'capital_outskirts'),\n",
    "    ('egg_chamber', 'breeding_territory')\n",
    "]\n",
    "\n",
    "print(\"Highest-weight paths:\")\n",
    "print(\"=\" * 50)\n",
    "for src, tgt in test_pairs:\n",
    "    if src in G_weighted and tgt in G_weighted:\n",
    "        path, weight = highest_weight_path(G_weighted, src, tgt)\n",
    "        if path:\n",
    "            print(f\"\\n{src} → {tgt}:\")\n",
    "            print(f\"  Path: {' → '.join(path)}\")\n",
    "            print(f\"  Weight: {weight:.4f}\")\n",
    "        else:\n",
    "            print(f\"\\n{src} → {tgt}: No path found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "The weighted graph perspective lets us find the most likely paths through the category.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## Part 6: Markov Chains and Stochastic Processes\n",
    "\n",
    "A probability-enriched category is essentially a Markov chain:\n",
    "- Objects = states\n",
    "- Hom(A, B) = transition probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize rows to get a proper transition matrix\n",
    "row_sums = Hom.sum(axis=1, keepdims=True)\n",
    "row_sums[row_sums == 0] = 1  # Avoid division by zero\n",
    "P = Hom / row_sums\n",
    "\n",
    "print(\"Transition matrix P (rows sum to 1):\")\n",
    "print(f\"  Row sums (should be ~1): min={P.sum(axis=1).min():.2f}, max={P.sum(axis=1).max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a random walk on the category\n",
    "def random_walk(P, start_idx, steps=10):\n",
    "    \"\"\"Simulate a random walk on the Markov chain.\"\"\"\n",
    "    path = [start_idx]\n",
    "    current = start_idx\n",
    "    \n",
    "    for _ in range(steps):\n",
    "        probs = P[current]\n",
    "        if probs.sum() == 0:\n",
    "            break\n",
    "        next_state = np.random.choice(len(probs), p=probs/probs.sum())\n",
    "        path.append(next_state)\n",
    "        current = next_state\n",
    "    \n",
    "    return path\n",
    "\n",
    "# Start from stakdur_territory\n",
    "start_obj = 'stakdur_territory'\n",
    "if start_obj in obj_to_idx:\n",
    "    start_idx = obj_to_idx[start_obj]\n",
    "    \n",
    "    print(f\"Random walks starting from '{start_obj}':\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i in range(5):\n",
    "        path_idx = random_walk(P, start_idx, steps=5)\n",
    "        path_names = [all_objects[idx] for idx in path_idx]\n",
    "        print(f\"  Walk {i+1}: {' → '.join(path_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute stationary distribution\n",
    "# This is the eigenvector of P^T with eigenvalue 1\n",
    "\n",
    "try:\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(P.T)\n",
    "    \n",
    "    # Find eigenvector with eigenvalue closest to 1\n",
    "    idx = np.argmin(np.abs(eigenvalues - 1))\n",
    "    stationary = np.real(eigenvectors[:, idx])\n",
    "    stationary = stationary / stationary.sum()  # Normalize\n",
    "    \n",
    "    # Top stationary probabilities\n",
    "    top_stationary = sorted(zip(all_objects, stationary), key=lambda x: -x[1])[:10]\n",
    "    \n",
    "    print(\"Stationary distribution (most likely states):\")\n",
    "    print(\"=\" * 50)\n",
    "    for obj, prob in top_stationary:\n",
    "        print(f\"  {obj}: {prob:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not compute stationary distribution: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "The stationary distribution tells us where a random walker spends most time. This reveals the \"attractors\" in the category.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "## Part 7: Connection to Attention Mechanisms\n",
    "\n",
    "In transformers, attention computes weighted relationships between tokens. This is exactly an enriched category:\n",
    "\n",
    "- Objects = tokens\n",
    "- Hom(A, B) = attention weight from A to B\n",
    "\n",
    "Attention is a probability-enriched functor!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate attention-like computation\n",
    "# Keys and queries from object features\n",
    "\n",
    "# Create simple features: one-hot encoding of morphism types received\n",
    "morphism_types = passages['morphism_type'].unique()\n",
    "n_types = len(morphism_types)\n",
    "type_to_idx = {t: i for i, t in enumerate(morphism_types)}\n",
    "\n",
    "# Feature matrix: rows = objects, columns = morphism types\n",
    "features = np.zeros((n, n_types))\n",
    "for _, row in passages.iterrows():\n",
    "    j = obj_to_idx[row['target_object']]\n",
    "    t = type_to_idx[row['morphism_type']]\n",
    "    features[j, t] += row['weight']\n",
    "\n",
    "print(f\"Object features: {features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute attention-like weights: softmax of dot products\n",
    "def attention_weights(features):\n",
    "    \"\"\"Compute attention matrix from features.\"\"\"\n",
    "    # Q, K = features (simplified: use same for both)\n",
    "    scores = features @ features.T  # (n, n)\n",
    "    \n",
    "    # Softmax per row\n",
    "    exp_scores = np.exp(scores - scores.max(axis=1, keepdims=True))\n",
    "    attention = exp_scores / exp_scores.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    return attention\n",
    "\n",
    "attention = attention_weights(features)\n",
    "\n",
    "print(\"Attention matrix computed\")\n",
    "print(f\"  Shape: {attention.shape}\")\n",
    "print(f\"  Row sums (should be 1): {attention.sum(axis=1)[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare attention weights to passage probabilities\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "k = 10  # Show first 10 objects\n",
    "\n",
    "# Original passage weights\n",
    "ax1 = axes[0]\n",
    "sns.heatmap(Hom[:k, :k], annot=True, fmt='.2f', cmap='Blues',\n",
    "            xticklabels=all_objects[:k], yticklabels=all_objects[:k], ax=ax1)\n",
    "ax1.set_title('Passage Weights (Original)')\n",
    "\n",
    "# Attention weights\n",
    "ax2 = axes[1]\n",
    "sns.heatmap(attention[:k, :k], annot=True, fmt='.2f', cmap='Oranges',\n",
    "            xticklabels=all_objects[:k], yticklabels=all_objects[:k], ax=ax2)\n",
    "ax2.set_title('Attention Weights (Learned)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-33",
   "metadata": {},
   "source": [
    "Both matrices are enriched Hom structures. The original is from observations; attention is computed from features. Both capture weighted relationships between objects.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-34",
   "metadata": {},
   "source": [
    "## Part 8: Enriched Functors\n",
    "\n",
    "A functor between enriched categories must preserve the enrichment structure.\n",
    "\n",
    "For probability-enriched categories:\n",
    "- F: C → D must satisfy: Hom_D(F(A), F(B)) ≥ Hom_C(A, B)\n",
    "- (The functor doesn't decrease probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an enriched functor: aggregation by region\n",
    "# Objects in same region map to same object\n",
    "\n",
    "regions = passages['region'].unique()\n",
    "print(f\"Regions: {list(regions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build regional Hom matrix\n",
    "# Objects = regions\n",
    "# Hom(R1, R2) = max weight of any passage from R1-object to R2-object\n",
    "\n",
    "# First, assign objects to regions based on where they're observed\n",
    "obj_to_region = {}\n",
    "for _, row in passages.iterrows():\n",
    "    obj_to_region[row['source_object']] = row['region']\n",
    "    obj_to_region[row['target_object']] = row['region']\n",
    "\n",
    "# Build regional Hom\n",
    "region_list = list(regions)\n",
    "n_regions = len(region_list)\n",
    "region_to_idx = {r: i for i, r in enumerate(region_list)}\n",
    "\n",
    "Hom_regional = np.zeros((n_regions, n_regions))\n",
    "\n",
    "for _, row in passages.iterrows():\n",
    "    src_region = row['region']\n",
    "    tgt_region = obj_to_region.get(row['target_object'], row['region'])\n",
    "    \n",
    "    i = region_to_idx[src_region]\n",
    "    j = region_to_idx.get(tgt_region, i)\n",
    "    \n",
    "    Hom_regional[i, j] = max(Hom_regional[i, j], row['weight'])\n",
    "\n",
    "np.fill_diagonal(Hom_regional, 1.0)\n",
    "\n",
    "print(f\"Regional Hom matrix: {Hom_regional.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize regional Hom\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "sns.heatmap(Hom_regional, annot=True, fmt='.2f', cmap='Greens',\n",
    "            xticklabels=region_list, yticklabels=region_list, ax=ax)\n",
    "ax.set_title('Regional Hom Matrix\\n(Enriched Functor from Objects to Regions)')\n",
    "ax.set_xlabel('Target Region')\n",
    "ax.set_ylabel('Source Region')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-38",
   "metadata": {},
   "source": [
    "The regional Hom matrix is the result of an enriched functor that aggregates objects by region. This is how we can \"zoom out\" while preserving categorical structure.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-39",
   "metadata": {},
   "source": [
    "## Part 9: Information Flow and Entropy\n",
    "\n",
    "The enriched structure tells us about information flow in the category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute entropy of outgoing transitions for each object\n",
    "def entropy(probs):\n",
    "    \"\"\"Compute Shannon entropy of a probability distribution.\"\"\"\n",
    "    probs = probs[probs > 0]  # Filter zeros\n",
    "    if len(probs) == 0:\n",
    "        return 0\n",
    "    probs = probs / probs.sum()  # Normalize\n",
    "    return -np.sum(probs * np.log2(probs + 1e-10))\n",
    "\n",
    "entropies = [entropy(P[i]) for i in range(n)]\n",
    "\n",
    "# Objects with highest/lowest entropy\n",
    "entropy_df = pd.DataFrame({'object': all_objects, 'entropy': entropies})\n",
    "entropy_df = entropy_df.sort_values('entropy', ascending=False)\n",
    "\n",
    "print(\"Highest entropy objects (most unpredictable outgoing):\")\n",
    "print(entropy_df.head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\nLowest entropy objects (most predictable outgoing):\")\n",
    "print(entropy_df.tail(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize entropy distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ax.bar(range(len(entropies)), sorted(entropies, reverse=True), color='steelblue', alpha=0.7)\n",
    "ax.set_xlabel('Object (sorted by entropy)')\n",
    "ax.set_ylabel('Entropy (bits)')\n",
    "ax.set_title('Entropy of Outgoing Transitions\\n(Higher = more unpredictable)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-42",
   "metadata": {},
   "source": [
    "High entropy objects have many possible outgoing passages with similar probabilities. Low entropy objects have one dominant passage.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-43",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Cost Enrichment\n",
    "\n",
    "Instead of probabilities, interpret weights as costs (where higher = more costly). Modify the composition rule to use addition instead of multiplication. Find the lowest-cost paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Use min instead of max, and + instead of *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-45",
   "metadata": {},
   "source": [
    "### Exercise 2: PageRank\n",
    "\n",
    "Implement PageRank on the passage category. This gives a different ranking than the stationary distribution—compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Use nx.pagerank(G_weighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-47",
   "metadata": {},
   "source": [
    "### Exercise 3: Morphism Type Weights\n",
    "\n",
    "Compute separate Hom matrices for each morphism type. How does the categorical structure differ between creature_passage and lifecycle_passage?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Filter passages by morphism_type before building Hom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-49",
   "metadata": {},
   "source": [
    "### Exercise 4: Multi-Head Attention\n",
    "\n",
    "Implement multi-head attention with 3 heads, where each head focuses on different morphism types. Compare the resulting attention patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Create different feature matrices for different morphism types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-51",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Discussion Questions\n",
    "\n",
    "1. Vold's weighted passages are observational. But attention weights are learned. Does this difference matter for the categorical interpretation?\n",
    "\n",
    "2. In physics, transition probabilities must be derived from dynamics. In Vold's framework, they're taken as primitive. Is this philosophically defensible?\n",
    "\n",
    "3. The enriched category framework applies to any \"valued\" relationships. What other domains (beyond probability and cost) might benefit from this perspective?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-52",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you learned:\n",
    "\n",
    "| Concept | What You Learned |\n",
    "|---------|------------------|\n",
    "| Enriched category | Hom(A, B) is a value, not just a set |\n",
    "| Probability enrichment | Morphisms have probabilities; composition multiplies |\n",
    "| Markov chains | Enriched categories as stochastic processes |\n",
    "| Attention | Enriched functor computing weighted relationships |\n",
    "| Stationary distribution | Long-run behavior of random walks |\n",
    "\n",
    "| Skill | Code Pattern |\n",
    "|-------|--------------|\n",
    "| Build Hom matrix | Initialize np.zeros, fill from data |\n",
    "| Normalize to transition | Divide rows by row sums |\n",
    "| Compute stationary | Eigenvector of P^T with eigenvalue 1 |\n",
    "| Attention weights | Softmax of dot products |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-53",
   "metadata": {},
   "source": [
    "## Course Conclusion\n",
    "\n",
    "You have now completed **Relational Foundations**, the first course in the Categorical Philosophy Series.\n",
    "\n",
    "You learned:\n",
    "1. **Passage Diagrams**: Categories as objects and morphisms\n",
    "2. **Classification Hierarchy**: Pre-orders as special categories\n",
    "3. **Composition and Identity**: The two fundamental operations\n",
    "4. **The Preservation Principle**: Functors as structure-preserving maps\n",
    "5. **The Probing Pattern**: Hom functor and representable functors\n",
    "6. **Coherent Shifts**: Natural transformations between functors\n",
    "7. **The Probing Theorem**: The Yoneda Lemma and its implications\n",
    "8. **The Language of Dens**: Language as a category\n",
    "9. **Weighted Passages**: Enriched categories and probabilistic structure\n",
    "\n",
    "### Vold's Final Word\n",
    "\n",
    "> *\"We began with a simple question: what are things? I proposed an answer: things are patterns of passages. An object is not defined by what it is, but by how other things pass through it, around it, to it. This is not mysticism—it is mathematics.*\n",
    ">\n",
    "> *The Probing Theorem shows that an object is fully determined by its probing pattern. Coherent shifts show how different perspectives relate. Weighted passages capture degrees of possibility. And the language of Dens shows that meaning itself arises from structure.*\n",
    ">\n",
    "> *Marden Krell asked: but what are the objects really? I say: there is no 'really.' There are only relationships. And mathematics—category theory—gives us the language to say this precisely.\"*\n",
    "> — Tessery Vold, \"Reflections on Passage,\" Year 920\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-54",
   "metadata": {},
   "source": [
    "## Credits\n",
    "\n",
    "**Source Material:** Tai-Danae Bradley, \"Category Theory and Language Models\" (Cartesian Cafe)\n",
    "\n",
    "**Densworld Integration:** The Relational Foundations course applies categorical concepts through the framework of Tessery Vold.\n",
    "\n",
    "**Learn more:** [buildLittleWorlds](https://github.com/buildLittleWorlds)\n",
    "\n",
    "---\n",
    "\n",
    "> *\"A passage is not merely a yes or no. The stakdur's daily hunt is a certainty; the grimslew's rare emergence is a whisper. Some paths are well-worn, others barely visible. To capture the world's structure, we must weight our passages. This is the final refinement: not just what passes, but how much.\"*\n",
    "> — Tessery Vold, \"Weighted Passages,\" Year 901"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
